{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes (Base) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After much stuff done with regression and also having a little try with decision trees, now it is time for much faster classification (and often much accurate) method. Called **Naive Bayes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "—<br>\n",
    "As other algorithmns, **Naive Bayes** also has some flavours. Actually there are many (and also one of them can be even used for regression task!) but here, we will see 2.\n",
    "\n",
    "1. Classification when the features are Continuous\n",
    "2. Classification when the features are Categorical\n",
    "\n",
    "The 1st is called ***Gaussain* Naive Bayes** and the second is **Naive Bayes** itself. <br>\n",
    "*(I also have got myself a question, that in real world, the dataset is in the mixture of both types of features, then which one can be used? - we will talk about that after discssing both of them individually here)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Intuition<br>— "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this classification algorithm, we are interested to calculate the probaility of the $\\text{hypothesis}(h)$ from the given data (features).\n",
    "\n",
    "For that we use *prior* knowledge learned by the model. The ***cool*** thing about this model is that, we are *not learning* like we used to. Like finding the parameters and then calculating the error and stuff, but here the term **lerning** means calculating the probabilities of each features' contributing to the decision for the target class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "—<br>\n",
    "We use our `prior` knowledge to calcualte the stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### THE FORMULA\n",
    "\n",
    "## $$ P(y | X) = \\frac {P(X | y) \\times P(y)} {P(X)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ $ P(X) $ is the normalizing term, hence mostly we get to see the formula $ P(y | X) = {P(X | y) \\times P(y)} $<br>\n",
    "→ We can also drop the $ P(y) $ if the number of instances of each target class is eqaual. Then we would endup with this only formula  $ P(y | X) = P(X | y) $ which is not the case everytime. <br>\n",
    "→ If the situation like above occurs, we would call it \"***Idiot Bayes***\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>—</center> <br>\n",
    "<b>That ↑ </b>formulae will be used for each target class (with given set for features) and from all of the classes which will be having the maximum prob, will win the game.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now**, as said we need to get things done in 2 ways based on the type of data (ie. Continuous or Categorical). We will get the workaround theoreticly here then in the upcoming books we will implement the code.\n",
    "\n",
    "Let's go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. When the data is `categorical`\n",
    "> It is called `Naive Bayes`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to calculate<br>— "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Class probability\n",
    "2. Conditional probability (for each feature, each class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will calcualte the probability for *each feature* because there is the strong assumption of bayes theorem that the features are ***independent***. \n",
    "\n",
    "*(Please, as I have told you—we are not going to talk all the stuff here because the goal of this notebook is to demonstrate the code; rather the theory. There is the physical book well annotated so read it there. Please.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "—"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class Probaility**\n",
    "\n",
    "### $$ P(class_1) = \\frac {\\text{count}(class_1)} {\\text{total classes}}$$\n",
    "*Well, a simple probability that we know.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conditional Probaility**\n",
    "\n",
    "### $$ P(X_1 = \\text{cat} | class_1) = \\frac {\\text{count}(X_1 = \\text{cat} \\text{ and } class_1)} {\\text{count}(class_1)}$$\n",
    "\n",
    "Here,<br>\n",
    "→ $ X_1 $ is the individual feature <br>\n",
    "→ $ cat $ is the unique category in that $ X_1 $ feature <br>\n",
    "→ We then calcualte the count when both of them happen in whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same equation can be simply written as ↓ <br>\n",
    "### $$ P(X_1 = A | B) = \\frac {\\text{count}(A \\cap B)} {\\text{count}(B)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cool!** By now, we have our model has learnt from the data. We can straightly go for the predictions with the new instances. By putting the values in the **naive bayes** formula just shown first. \n",
    "\n",
    "***NOTE***: As said, we will not calculate $ P(X) $ here, because it is the normalizing term. We could if we want. We will see how later. Please understand the situation first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREDICTION\n",
    "\n",
    "### $$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "class_1 = & P(X_1 | class_1) \\\\\n",
    "        & \\times P(X_2 | class_1) \\\\\n",
    "        & ... \\\\\n",
    "        & \\times P(X_n | class_1) \\\\\n",
    "        & \\times P(class_1) \n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. When the data is `continuous`\n",
    "> It is called `Gaussian Naive Bayes`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to calculate<br>— "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Class probability\n",
    "2. Conditional probability (for each feature, each class)\n",
    "    - PDF\n",
    "        - mean\n",
    "        - std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, you see there is a little hierarchy there. We need to find the *conditional probs* and for that we have to calcualte the PDF and for that we need to have mean and std for each feature and target pairs.\n",
    "\n",
    "Let's see which formulaes are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "—<br>Still here we will plug the values in the $ P(y | X) = \\frac {P(X | y) \\times P(y)} {P(X)} $ formula, but this time we will use the $ PDF $ function to do that for us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PDF\n",
    "\n",
    "## $$ PDF = \\frac{1} {\\sqrt{2\\pi} \\times \\sigma} \\times e^{-\\left(\\frac{(x - \\bar x)^2} {2 \\times \\sigma^2}\\right)}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoooh! Pretty big one, isn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 2 things we need to find, <br>$ \\bar x $ = mean <br> $ \\sigma $ = standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class Probaility**\n",
    "\n",
    "### $$ P(class_1) = \\frac {\\text{count}(class_1)} {\\text{total classes}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREDICTION\n",
    "\n",
    "### $$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "class_1 = & PDF(X_1 | class_1) \\\\\n",
    "        & \\times PDF(X_2 | class_1) \\\\\n",
    "        & ... \\\\\n",
    "        & \\times PDF(X_n | class_1) \\\\\n",
    "        & \\times P(class_1) \n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the PDF will take the mean and std of each pair of feature and class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great!\n",
    "Next up, we will actually implement them from scratch and they will be distributed in 3 notebooks (3rd for mixed features)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
