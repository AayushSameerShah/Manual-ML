{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Forest of Mathematical Trees "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man! Whenever you try to read about the random forests, you are most likely to read this term \"*most powerful*\". Yes? because it is. But **how**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>• Random Forest is ***not*** a model • </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arg! A huge statement from my side huh? Yes it is. Random forest (I don't know why I want to write \"Forrest\") is not the model. It is **the collection** of trees — decision trees, which are indeed the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><b>Q.</b></u> Okay, but why **Random?**:<br>\n",
    "<u><b>A.</b></u> Because it makes the decision trees by picking the random instances from the model (also with replacement) and randomly selected features as well. So the *trees* grown in the *forest* are picked from randomly chosed *seeds* or features and instances, thus → **\"Random (ly picked features and instances to make the trees which combinely makes) forest\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "—*Am I being poetic?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font size=6>N</font></b>ow, things don't happen entierly randomly. They have to follow certain procedure. There has to be some kind of algorithm to pick random seeds, arange them, make trees, make prediction out of them etc. So what is that? Here, we (the statastitians -arg! how to spell statisticians?) introdced the proceure which is called \"***Bootstrap Aggregation*** (bagging—for short)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap<br>—\n",
    "Before we try to understand the Bootstrap *with* \"Aggregation\", I would like to first explain \"Bootstrap\" only. **So, what is bootstrap?** (I know you may have heard if you are the web developer, but here it is different thing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the statasticians' words:\n",
    "> ### Bootstrap is a *powerful statistical method* for estimating a quantity from a data sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(I know the word \"*powerful*\" comes a lot here, but make it as a habbit, it has to be)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the my words:\n",
    "> ### Bootstrap is a procdure *to resample* the elements from the sample for *many times* (also with replacement) **to improve the estimate** from the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Again, didn't get it? No worries! It is always better to explain with examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bootstrap Example**: <br>\n",
    "Imagine we have conducted a survey of students' heights. We have got the measures. Total measures are 100. Now, we will get the estimate of the heights (in this case we will take a mean). So, we will get the heights' estimate from 100 samples.\n",
    "\n",
    "- Can we use this estimate for whole population? \n",
    "- Is this the best we can do?\n",
    "- Is this estimate enough for us?\n",
    "\n",
    "We can do better. Our sample is so small so estimating from this for a population may well lead us to poor estimate. It is where the **bootstraping** comes in.\n",
    "\n",
    "—Then, what we can do is, we *resample* the elements from the sample. So for this example we will take 1000 *samples* ***from*** *the sample* which, we better call it \"sub-sample\". And we will take the desired number of sub-samples. Let's say we are taking 5 sub-samples from our sample (100).\n",
    "\n",
    "Thus, we will end up with 1000 x 5 = 5000 elements. Remember, they are picked randomly with replacement from those 100 elements. After that, we can take the mean (or other operation, but here we are taking mean) of all those individual 5 sub-samples and then again take mean of those 5 means (mean of means). \n",
    "\n",
    "The final mean value will be more reliable. And this is how bootstraping works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap Aggregation *(bagging)*<br>— "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, we have already talked about the \"Aggregation\" part just above in Bootstraping section (kind of). Yes, **when** we took the sub-samples from the sample, and took the mean of those 5 sub-samples it was bootstraping (which involves the randomly data selection, splitting, operating etc. and **when** we took the mean of means, we did \"Aggregation\". You know, it was like combining all of the inputs from bootstraped output.\n",
    "\n",
    "So, are we done? Actually not. The bagging (I would use this bagging term instead of BA) has more into it, than what we have just demonstrated above. Bagging is more used in machine learning rather than just getting the mean out of some samples. It also involves a bit more steps. So, let's discuss what are they."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bootstrap Aggregation** from the book's words:<br>\n",
    "> #### It is an ensamble technique that combines the predictions from multiple ML models together to make more accurate prediction than any other individual model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, we create multiple trees. Say, we have a dataset. Now from that dataset we will take 5 sub-samples which will get fed to the 5 decision trees. The sub-sample will be chosen randomly. \n",
    "\n",
    "- The decision trees are **sensitive** to the specific data on which they are trained. If the training data is changed, the resulting decision tree can be different and in turn the predictions will be different. \n",
    "\n",
    "- **When** bagging with decision trees, ***we are less concerned*** about the individual trees' overfitting the training data.**For this reason** the individual decision trees are grown deep ***and*** the trees are **not pruned**. Which will result in **high variance**. *(So it says that **overfitting is the good thing** here)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have found our 1st hyperparameter here: `Number of Trees`. We should control the number of trees to train individually which results in great contribution in voting of the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> . . . </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "—The term **Ensamble**: Ensamble is the **family** of the techniques which involves making multiple models, providing them individual dataset, training individually and then instead of predicting from some single model, predict from all models and get the result somehow. And our **Bootstrap Agg** is the part of this **ensamble** family.\n",
    "\n",
    "Ensamble involves other techniques like - Adaboost, Gradient Boost, Xgbost etc. (Adaboost will be discussed in the very next book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Plot Twist.\n",
    "The procedure we discussed just above ↑ about the Bootstrap Aggregation **is not what happens in the random forest**. If we follow that same procedure and create our algorithm from that, then we will not endup with the random forest—but instead that will be called ***bagged decision trees***. Yes, I know a new term but it will be called that. \n",
    "\n",
    "Now, in reality we don't use that \"bagged decision trees\". The reason behind that is the decision trees use the *greedy* aproach to find the split point which results in **high correlation in the output**.\n",
    "<br>—<br>\n",
    "Let me put in this way, <br>\n",
    "If we make **no change** in the above describe process and use it as it is, what will happen is, let's say we chose `5` as the number of trees to be grown. So, 5 sub-samples will be chosen. Okay? Then, as said the decision tree's algorithm is *greedy* which means, it will **check each feature**, calculate the gini index and then find the best split point **out of *all* features**. This ***will result in the similar looking and similar structured dicision trees*** (because you think, 3rd feature is the best split point mostly, then *even if* the individual trees are trained on sub-samples but *there is the high chance* that all sub-samples may include that 3rd feature. And if our algorithm goes and checks gini index in all features, the 3rd feature will always trun out to be the *best split point **in every** individual split point*! (Are you catching the problem?)\n",
    "\n",
    "This will eventually result in the high correlated prediction of individual decision trees! This is wrong, we need to stop this. And this is done with a small tweak in the algorithm, a change which makes it from \"Bagged Decision Trees\" → \"Random Forest\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is to choose *how many*** features to check. Here comes our new hyperparameter `m`. It is the number of features to check on each split point. \n",
    "\n",
    "*(Again, a new reason which contributes to the term **Random** in random forest (`m` features will be selected randomly))*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many `m`?\n",
    "See, random forest can be used for both, **classification** and **regression**. So, the default `m` value has some rule of thumb.\n",
    "\n",
    "- For classification → $ m = \\sqrt{p} $\n",
    "\n",
    "- For regression → $ m = \\frac{p}{3}$\n",
    "\n",
    "*(Where $ p $ is number of features)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are we done?\n",
    "Aah, it was too much right? I think for the base of Random Forest, it is enough for now. In the next book, my boy Ayush, we will look for the Adaboost (another ensemble technique).\n",
    "\n",
    "See you there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
