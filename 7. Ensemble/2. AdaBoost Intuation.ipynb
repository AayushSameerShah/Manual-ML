{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Weaks. Get Stronger.\n",
    "*I am talking about AdaBoost.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Howdy Aayush!** <br>This is the second notebook for you to understand the *2nd* ensemble method—**AdaBoost** (yeah, 'A' and 'B' capital)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "—"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, before going right into the explanation and formulae, I would like to give you a little background of what an ensamble is. Actually we have discussed in detail what that is in the previous notebook, but still as a refresher if you are coming here directly.\n",
    "\n",
    "> So, **ensemble** is a ***family*** of methods to split the data somehow (in sub-samples) — train different models on them — and while predicting, predict from all models and combinely make a final prediction. It is what the ensemble is.\n",
    "\n",
    "We have gone through the 1st technique called \"Bootstrap Aggregation\". Now, it is time to checkout \"AdaBoost\". Keep that in your mind that, these technique are very advanced. What I am going to discuass will give you the interoduction and the hint of what will be happening. Because the book did the same. And that is more than enough!\n",
    "\n",
    "Don't worry, let's get started ∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> . . . </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting<br>— "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we try to understand what the \"AdaBoost\" is, we need to understand what *boost* is in \"Ada**Boost**\". \n",
    "\n",
    "*(I think you might relate this as the pattern that we followed in the previous notebook. We first discussed the \"Bootstrap\" to understand \"Bootstrap Aggregation\". Likewise here the same, we will understand the \"Boosting\" first)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **Boosting** is a ***family*** of algorithms which builds a mdel from the training data, then creates a second model that attempts to correct the errors from the first model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means we will keep adding the models sequentially and each new model will be better than the previous one by correcting the errors made by the previous model. \n",
    "\n",
    "This addition of model is continued till **`either`** the maximum number of required models has reached **`or`** no improvment is needed in the model. And this concept (of getting Stronger from Weak) is called ***boosting***.\n",
    "\n",
    "The process looks like this ↓\n",
    "    \n",
    "    ┌─┐               ┌─┐\n",
    "    │W│   ┌─┐         │A│         ┌─┐\n",
    "    │e│   │W│   ┌─┐   │v│   ┌─┐   │B│   ┌─┐\n",
    "    │a│   │e│   │W│   │e│   │G│   │e│   │B│\n",
    "    │k│ → │a│ → │e│ → │r│ → │o│ → │t│ → │e│\n",
    "    │e│   │k│   │a│   │a│   │o│   │t│   │s│\n",
    "    │s│   │e│   │k│   │g│   │d│   │e│   │t│\n",
    "    │t│   │r│   └─┘   │e│   └─┘   │r│   └─┘\n",
    "    └─┘   └─┘         └─┘         └─┘\n",
    "    \n",
    "     M1    M2    M3    M4    M5   M6     M7\n",
    "     \n",
    "        ←— Weak              Strong —→"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(You know, I love ascii art)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't be confused by the heirarchy. Let's have a look at the hierarchy for simplicity.\n",
    "<pre>           ┌─────────┐\n",
    "       ┌───┤Ensemble ├───┐\n",
    "       │   │(Family) │   │\n",
    "       │   └─────────┘   │\n",
    "       │                 │\n",
    "       ▼                 ▼\n",
    "┌──────────────┐    ┌─────────────────┐\n",
    "│ Bootstraping │    │Boosting         │\n",
    "│   (Method)   │    │(Family & Method)│\n",
    "└─────┬────────┘    └────┬────────────┘\n",
    "      │                  │AdaBoost\n",
    "      │Random Forest     │\n",
    "                         │Gradient Boost\n",
    "                         │\n",
    "                         │XGBoost</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost<br>— "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I didn't understand what is the underlying difference between other techniques *(between AdaBoost / GBoost / XGBoost etc.)* as I have not explored yet but, let's just discuss the AdaBoost here. It is simple and follows the Boosting method directly.\n",
    "\n",
    "<u><b>Q.</b></u> What the freak **Ada** is?<br>\n",
    "<u><b>A.</b></u> **Ada** stands for **Addition** as we ***add*** the models which I thought. But **that is not**. Ada stands for **Adaptive**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike Random Forest, AdaBoost **can be used to improvise** ***any*** algoritim. <br>\n",
    "But as the author says:\n",
    "> \"Other models (other than decision trees) that achieve accuracy **just above random chance** on the clasification problem. So, the most suited and <u>therefore most common</u> algorithm used with AdaBoost are **decision trees**.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems correct, isn't it? And note that, AdaBoost was created (and was proven to be the best ) for the binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's directly talk about the algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> . . . </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6><b>A</b></font>daBoost commonly uses the decision trees as we saw just above, the trees are most commonly too short of just `1` level deep. Which we call a *Decision Stump*. It just splits data with one check. So we can conclude that AdaBoost's decision trees are commonly decision stumps.\n",
    "\n",
    "Now, we can implement this small decision trees here to demonstrate the AdaBoost, but I would rather work with other algoritms. But for now, in this theory book - we will talk about the steps and formulaes, and then in the second book we will take a look at the algorithm in coded version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> . . . </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The pseudo code**:\n",
    "1. **Initialize the weights** to each row in the dataset.\n",
    "2. **Make** the model.\n",
    "2. **Predict** with model.\n",
    "3. Get the **perror**. (yes \"p\"error)\n",
    "4. Get the **misclassification rate**.\n",
    "5. Calcualte the **stage** value.\n",
    "6. Update the **weights** for each row in the dataset.\n",
    "7. Make **new** model.\n",
    "8. Repeat the steps from 2 to 8 till *n* models are created.\n",
    "9. After all 8 steps, we will do some ***ensambling***. Our model is ready to predict.\n",
    "10. Predict using all model's stage values and predictions.\n",
    "11. Enjoy, much accurate model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALIZE WEIGHTS\n",
    "\n",
    "# $$ w = \\frac {1} {n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GET CLASSIFICATION RATE<br>(without accounting weights - NON USABLE)\n",
    "\n",
    "## $$ \\text{MisClf Rate} = \\frac {\\text{correct} - N} {N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GET CLASSIFICATION RATE<br>(with accounting weights - USABLE)\n",
    "\n",
    "## $$ \\text{MisClf Rate} = \\frac {\\sum(w_i \\times \\text{perror}_i)} {\\sum {w_i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PERROR?\n",
    "\n",
    "### `perror = 0   IF   y == Pred` <br>  `perror = 1   IF   y != Pred`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  STAGE VALUE\n",
    "\n",
    "## $$ \\text{stage} = ln\\left( \\frac {1 - \\text{MisClf Rate}} {\\text{MisClf Rate}} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UPDATE WEIGHTS\n",
    "\n",
    "# $$ w_i = w_i \\times e^{\\text{stage} \\times \\text{perror}_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amzaing!\n",
    "I think we are ready to implement our own AdaBoost, so we will do that in the next book."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
