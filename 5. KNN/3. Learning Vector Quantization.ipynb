{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LVQ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*A downside of KNN was that, you have to store entire dataset*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is LVQ?<br>—\n",
    "LVQ is of course the short form of **Learning Vector Quantization** and is the extention of KNN works on artificial neural nets.\n",
    "\n",
    "###### \n",
    "\n",
    "## What's the point?<br>—\n",
    "Unlike KNN, this LVQ *learns* from the data. It requires training. It represents the entire dataset in some format. And this works faster than KNN, why in just a bit.\n",
    "\n",
    "###### \n",
    "\n",
    "## How to represent?<br>—\n",
    "The representation of LVQ model is the collection of `codebooks`. Now, codeblock in **our** world we call it `neurons`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><font size=6>H</font></b>ere we predict the class in the same wat as we do for KNN (by voting, of course). For each new instance we search through all neuron vectors for the K most similar instances. \n",
    "\n",
    "Here too we can use multiple (from `8`) distance metrics. For simplicity, we will use our *Euclidean Distance*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Euclidean Distance\n",
    "\n",
    "# $$ ED = \\sqrt{\\sum (A - B)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the learning goes?<br>—\n",
    "\n",
    "The goal of this algorithm is **not to** store and predict from whole dataset, otherwise KNN was the better choice. Here, we have to ***learn*** from the data so that we can summarize the data to som extent. So, here we choose **number of neuron vectors** or **number of layers in the NN** (like 10, 100, 500).\n",
    "\n",
    "**1st Hyperparameter** = `n_layers`\n",
    "\n",
    "At starting, the algorithm rendomly initializes all neurons with<br>\n",
    "**Either**<br>\n",
    "Some data instances from data.<br>\n",
    "**Or**<br>\n",
    "Some data generate from the same scale as the training data has.\n",
    "\n",
    "—<br>\n",
    "There is a term in the notebook, which is quite not fimiliar to me by the time I am writing this, is \"moving closer or away of a layer\". Yes. It goes like:\n",
    "> If the layer has the same output as the training instance, that layer is moved closer to the training instance. If it does not match, it is moved further away.\n",
    "\n",
    "Of course it is the thing to explore when we will implement how to... but for now let's get it going.The amount of getting closer/away is determined by the LearningRate parameter. So, here we have got our new hyperparameter.\n",
    "\n",
    "**2nd Hyperparameter** = `Learning Rate`\n",
    "\n",
    "###### \n",
    "\n",
    "#### If layer moves ***closer***\n",
    "## $$ \\text{layer} = \\text{layer} + \\text{LearningRate} \\times (x - \\text{layer}) $$\n",
    "\n",
    "###### \n",
    "\n",
    "#### If layer moves ***further***\n",
    "## $$ \\text{layer} = \\text{layer} - \\text{LearningRate} \\times (x - \\text{layer}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cool thing about this LearningRate is that, here in this algorithm - we modify our learning rate as the epoch is done. What I mean by that is, the learning rate changes and gets lower and lower as the number of epoch increases.\n",
    "\n",
    "See... for an example we have chosen **10** epochs. And the learning rate `0.1`, then for the first epoch the learning rate is `0.1` but on the second it *will get lower*, on the third *lower* than second... on the **10th** it will be *lowest* and will make **minimal to no change** in the learned parameters. (Hey, can I use ther term \"parameters\" here? I mean it is extention of KNN and KNN is non parametric!) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update LearningRate\n",
    "\n",
    "## $$ \\text{LearningRate} = \\text{LearningRate} \\times \\left( 1 - \\frac{n^{th} \\text{Epoch}} {\\text{TotalEpochs}}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's all for the intuation\n",
    "Next up, we will build that LVQ from scratch and have a better look at it how it works... and probably will compare the speed and accuracy for KNN and LVQ. \n",
    "\n",
    "See you there bro."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
